{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4855050",
   "metadata": {},
   "source": [
    "# NLP 공부하기 - DAY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa0fb2",
   "metadata": {},
   "source": [
    "# 📘 DAY 1 — NLP 기본 개념 잡기 (완전 기초)\n",
    "\n",
    "오늘 목표는 **NLP가 뭔지, 텍스트를 어떻게 숫자로 바꾸는지, NLP 전체 흐름이 어떻게 돌아가는지** 감을 잡는 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "# 1️⃣ NLP는 무엇을 하는가?\n",
    "\n",
    "✔️ **사람의 언어를 숫자로 바꿔서 컴퓨터가 이해하게 만드는 분야**\n",
    "\n",
    "컴퓨터는 결국 **숫자(벡터)**만 이해한다는 점이 핵심이에요.\n",
    "\n",
    "예시:\n",
    "\n",
    "“배가 아프다” → [0.12, -0.45, 1.22, …]\n",
    "\n",
    "“복통” → [0.14, -0.47, 1.18, …]\n",
    "\n",
    "이런 식으로 문장이나 단어를 숫자 벡터로 변환해서 머신러닝/딥러닝 모델에 넣습니다.\n",
    "\n",
    "---\n",
    "\n",
    "# 2️⃣ NLP 전체 구조(파이프라인) 한 번에 보기\n",
    "\n",
    "NLP 기본 흐름은 아래와 같습니다:\n",
    "\n",
    "```\n",
    "텍스트 → 전처리 → 토큰화 → 숫자화 → 모델 → 결과\n",
    "\n",
    "```\n",
    "\n",
    "조금 더 자세히 풀면:\n",
    "\n",
    "1. **텍스트 수집**\n",
    "    \n",
    "    (뉴스, 리뷰, 의료 기록 등)\n",
    "    \n",
    "2. **전처리**\n",
    "    - 소문자 변환\n",
    "    - 특수문자 제거\n",
    "    - 정규표현식으로 정리\n",
    "3. **토큰화(Tokenization)**\n",
    "    \n",
    "    긴 문장을 쪼개서 단어, 형태소, subword로 나누기\n",
    "    \n",
    "4. **벡터화(숫자로 바꾸기)**\n",
    "    - TF-IDF\n",
    "    - Word2Vec\n",
    "    - BERT 임베딩\n",
    "5. **모델에 입력**\n",
    "    - Naive Bayes\n",
    "    - LSTM\n",
    "    - BERT\n",
    "    - GPT 모델 등\n",
    "6. **출력**\n",
    "    - 감정 분석\n",
    "    - 요약\n",
    "    - 번역\n",
    "    - 질병 코드 분류(ICD)\n",
    "\n",
    "이 전체를 한 번 머릿속에 넣어두면 제일 좋아요.\n",
    "\n",
    "---\n",
    "\n",
    "# 3️⃣ 텍스트는 어떻게 숫자로 바뀌는가? (중요!)\n",
    "\n",
    "NLP를 이해하는 핵심은 바로 이것입니다.\n",
    "\n",
    "**텍스트 → 숫자 벡터로 바뀌는 과정**\n",
    "\n",
    "✔️ 예시 문장\n",
    "\n",
    "“나는 간단한 NLP를 배우고 있다”\n",
    "\n",
    "### ➤ Step 1: 토큰화\n",
    "\n",
    "→ [\"나\", \"는\", \"간단한\", \"NLP\", \"를\", \"배우고\", \"있다\"]\n",
    "\n",
    "### ➤ Step 2: 각 단어를 숫자로 매핑\n",
    "\n",
    "원하는 방식에 따라 다름:\n",
    "\n",
    "### (1) 단순한 Bag-of-Words\n",
    "\n",
    "\"나\" → 1번, \"간단한\" → 2번 등\n",
    "\n",
    "하지만 단순해서 단점 많음.\n",
    "\n",
    "### (2) TF-IDF\n",
    "\n",
    "문서에서 중요한 단어일수록 큰 숫자 부여.\n",
    "\n",
    "### (3) Word2Vec / Embeddings\n",
    "\n",
    "유사한 뜻끼리 가깝게 위치하게 학습\n",
    "\n",
    "반복해서 강조되는 이유:\n",
    "\n",
    "→ **모든 근대 NLP는 이 임베딩 개념 위에서 돌아간다**\n",
    "\n",
    "### (4) BERT / GPT 임베딩\n",
    "\n",
    "훨씬 똑똑한 문맥 기반 벡터\n",
    "\n",
    "Context-aware vector\n",
    "\n",
    "→ “은행(돈)”과 “은행(강가)”를 구분해냄\n",
    "\n",
    "---\n",
    "\n",
    "# 4️⃣ NLP의 주요 Task(어떤 문제를 풀까?)\n",
    "\n",
    "처음에는 아래 4개만 알면 충분해요.\n",
    "\n",
    "## ✔️ 1. 텍스트 분류 (Text Classification)\n",
    "\n",
    "- 감정 분석(긍정/부정)\n",
    "- 스팸 분류\n",
    "- 의료 기록 → 질병 코드(ICD) 분류\n",
    "\n",
    "## ✔️ 2. 개체명 인식(NER)\n",
    "\n",
    "문장에서 의미 있는 단어 위치 찾기\n",
    "\n",
    "예: 이름, 날짜, 질병명, 약품명 등\n",
    "\n",
    "## ✔️ 3. 문장 생성(Generation)\n",
    "\n",
    "- 번역\n",
    "- 요약\n",
    "- GPT처럼 질문에 답변 생성\n",
    "\n",
    "## ✔️ 4. 검색(Retrieval)\n",
    "\n",
    "- RAG에서 핵심\n",
    "- 주어진 질문과 가장 관련 있는 문서 찾기\n",
    "\n",
    "---\n",
    "\n",
    "# 5️⃣ 오늘의 실습(기초) — 파이썬으로 아주 간단하게\n",
    "\n",
    "오늘은 아주 간단한 기초만 해볼게요.\n",
    "\n",
    "### ✔️ 목표\n",
    "\n",
    "한 문장을 토큰화하고, 단순한 벡터로 바꿔보기\n",
    "\n",
    "\n",
    "# 6️⃣ 오늘 정리\n",
    "\n",
    "오늘 Day 1에서는 다음 개념을 잡았습니다:\n",
    "\n",
    "- NLP는 “텍스트를 숫자로 바꾸는 기술”\n",
    "- 전체 파이프라인 구조 이해\n",
    "- 토큰화 → 임베딩 → 모델의 기본 흐름\n",
    "- NLP의 주요 태스크 종류\n",
    "- 간단한 벡터화 실습\n",
    "\n",
    "이 정도면 NLP 입문은 성공이에요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76f4b7",
   "metadata": {},
   "source": [
    "# 실습. Bag-of-Words 방식의 백터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca053bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 사전: {'나는': 2, 'nlp를': 1, '배우고': 3, '있다': 4, 'nlp는': 0, '재미있다': 5, '컴퓨터를': 7, '좋아한다': 6}\n",
      "벡터화 결과:\n",
      " [[0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"나는 NLP를 배우고 있다.\",\n",
    "    \"NLP는 재미있다\",\n",
    "    \"나는 컴퓨터를 좋아한다.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(\"단어 사전:\", vectorizer.vocabulary_)\n",
    "print(\"벡터화 결과:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee61a7e",
   "metadata": {},
   "source": [
    "**<단어 사전>**\n",
    "\n",
    "왜 인덱스가 순차적이지 않아보이는 거지?     \n",
    "\n",
    "CountVectorizer에서 단어 사전의 인덱스는 사전순(알파벳/유니코드 순)으로 정렬된 후 0부터 부여        \n",
    "> 등장 순서대로 인덱스를 주는 것이 아니라, 정렬한 뒤 인덱스를 붙인다.       \n",
    "\n",
    "\n",
    "**정렬 기준 → 유니코드(UTF-8) 기반 사전순**\n",
    "실제로 CountVectorizer는 내부적으로 이렇게 정렬:            \n",
    "1. 전처리(소문자 변환, 특수문자 제거 등)\n",
    "2. 토큰화\n",
    "3. 단어 목록을 정렬(sorted)\n",
    "4. 정렬된 순서대로 0,1,2,3,… 인덱스 부여        \n",
    "\n",
    "\n",
    "위 순서를 적용해보면,       \n",
    "1. 단어 목록을 리스트로 정렬        \n",
    "['nlp는', 'nlp를', '나는', '배우고', '있다', '재미있다', '좋아한다', '컴퓨터를']        \n",
    "\n",
    "2. CountVectorizer는 여기에 차례대로 인덱스를 부여      \n",
    "nlp는 → 0           \n",
    "nlp를 → 1           \n",
    "나는 → 2            \n",
    "배우고 → 3          \n",
    "있다 → 4            \n",
    "재미있다 → 5            \n",
    "좋아한다 → 6            \n",
    "컴퓨터를 → 7            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0887a",
   "metadata": {},
   "source": [
    "**<벡터화 결과>**\n",
    "\n",
    "인덱스 순서상 : 0:NLP는 1:NLP를 2:나는 3:배우고 4:있다 5:재미있다 6: 좋아한다 7: 컴퓨터를       이다.\n",
    "그래서,         \n",
    "\"나는 NLP를 배우고 있다\" 의 문장을 벡터화 하면,     \n",
    "|단어 |등장 유무|값 \n",
    "|---|---|---\n",
    "|NLP는|X|0\n",
    "|NLP를|O|1\n",
    "|나는|O|1\n",
    "|배우고|O|1\n",
    "|있다|O|1\n",
    "|재미있다|X|0\n",
    "|좋아한다|X|0\n",
    "|컴퓨터를|X|0\n",
    "\n",
    "\n",
    "        \n",
    "> [0, 1, 1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb31644",
   "metadata": {},
   "source": [
    "**그렇다면, 단어 사전의 크기가 100개라면 백터 값이 100개가 되는 건가?**\n",
    "\n",
    "✅ 단어 사전(vocabulary)이 100개라면?       \n",
    "\n",
    "CountVectorizer 기준:\n",
    "각 문장 → 길이가 100인 벡터로 변환된다.     \n",
    "각 위치(0~99)는 단어 사전의 단어 하나를 의미한다.       \n",
    "값은 그 단어가 몇 번 등장했는지(count)를 나타낸다.      \n",
    "\n",
    "즉,     \n",
    "단어 사전 크기 = 벡터의 차원 수     \n",
    "이게 Bag-of-Words의 핵심 구조다.        \n",
    "\n",
    "🔍 예시로 보면      \n",
    "단어 사전 크기: 100     \n",
    "문장 1 → [0, 1, 0, 0, 3, 0, ... 총 100개]       \n",
    "문장 2 → [1, 0, 0, 5, 0, 0, ... 총 100개]       \n",
    "문장 3 → [0, 0, 0, 0, 1, 0, ... 총 100개]       \n",
    "\n",
    "📌 추가로 중요한 점\n",
    "✔ 단어 사전 크기가 커질수록 → 벡터 차원도 아주 커진다\n",
    "\n",
    "→ 이걸 고차원 sparse vector라고 부른다.\n",
    "→ 대부분 0이기 때문에 “sparse matrix” 형태로 저장한다.\n",
    "\n",
    "✔ 희소성이 높은 문제 → 모델 성능 저하 or 과적합 발생 가능\n",
    "→ 그래서 다음 방법들이 자주 사용된다:\n",
    "- TF-IDF\n",
    "- n-gram 제한\n",
    "- Stopwords 제거\n",
    "- 단어 빈도 제한(min_df, max_df)\n",
    "- 차원 축소(PCA, SVD / LSA)\n",
    "- 워드임베딩(word2vec, fastText, GloVe)\n",
    "- BERT 계열과 같은 contextual embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
